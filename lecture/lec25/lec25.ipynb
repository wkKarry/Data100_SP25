{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 25 â€“ Data 100, Spring 2025\n",
    "\n",
    "Data 100, Spring 2025\n",
    "\n",
    "[Acknowledgments Page](https://ds100.org/sp25/acks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from ds100_utils import *\n",
    "import plotly.express as px\n",
    "\n",
    "# Reduce number of sigfigs shown by numpy\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "# Reduce number of sigfigs shown by pandas\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with SVD\n",
    "\n",
    "Looking at this `rectangle` data, we can see that it is rank 3, since perimeter\n",
    "is a linear combination of width and height.\n",
    "\n",
    "Area is not a linear combination of width and height, but we might surmise that\n",
    "area does not provide a lot of additional information beyond width and height.\n",
    "Let's see if PCA picks up on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectangle = pd.read_csv(\"data/rectangle_data.csv\")\n",
    "rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Center the Data Matrix $X$\n",
    "\n",
    "Keep in mind that `sklearn` centers data by default when fitting PCA. Here,\n",
    "we are doing the linear algebra by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = rectangle - np.mean(rectangle, axis = 0)\n",
    "X_centered.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In situations where the units are on different scales, it is useful to normalize (i.e., standardize) the data before performing SVD. \n",
    "This can be done by dividing each column by its standard deviation.\n",
    "\n",
    "- This puts every column on a standard deviation scale. A value of 1 implies the entry is 1 standard deviation higher than its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_centered / np.std(X_centered, axis = 0)\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Get the SVD of standardized $X$\n",
    "\n",
    "<img src=\"img/svd.png\" alt=\"SVD\" style=\"width:700px;height:auto;\">\n",
    "\n",
    "The `np.linalg.svd` function computes the SVD of an inputted `X` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(X, full_matrices = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `full_matrices = False` truncates the number of columns of U to the\n",
    "> rank of X to avoid unnecessary computation. PCA does not use more columns of U\n",
    "> than the rank of X. This is sometimes called the \"economy\" SVD. The slides use the dimensions of the economy SVD.\n",
    "> Don't worry about these details for Data 100! Just include the argument.\n",
    "\n",
    "SVD dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of U\", U.shape)\n",
    "print(\"Shape of S\", S.shape)\n",
    "print(\"Shape of Vt\", Vt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First 10 rows of U. The 4 cols are the latent features but expressed as length 1 vectors.')\n",
    "print(U[:10, :])\n",
    "print()\n",
    "\n",
    "print('S. The 4 singular values that \"scale up\" the 4 cols of U into the 4 latent features (Z).')\n",
    "print(S)\n",
    "print()\n",
    "\n",
    "print('Vt. The 4 rows are the principal components. \"Recipes\" for combining the 4 real features into each of the 4 latent features. Rows and columns are unit vectors.')\n",
    "print(Vt)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S$ is a little different in `NumPy`. Since the only useful values in the diagonal matrix $S$ are the singular values on the diagonal axis, only those values are returned and they are stored in an array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the diagonal elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.diag makes a diagonal matrix from the vector S\n",
    "Sm = np.diag(S)\n",
    "Sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the contribution to the total variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(S**2 / np.sum(S**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that 72\\% and 26\\% of the variance is in the first two PC dimensions, respectively, which makes sense since rectangles are largely described by height and length.\n",
    "\n",
    "- Area is not a linear combination of height and length, so its contribution is non-zero but very small.\n",
    "\n",
    "- Perimeter is a linear combination of height and length, so its corresponding singular value is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The information below is only relevant if you print out all digits with `numpy`.** We set an option at the top of the notebook to only shown two decimal places.\n",
    "\n",
    "Hmm, looks like are four diagonal entries are not zero. What happened?\n",
    "\n",
    "It turns out there were some numerical rounding errors, but the **last value is so small ($10^{-15}$) that it's practically $0$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(S[3], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.round(np.diag(S),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Computing Approximations to the Data\n",
    "\n",
    "Let's try to approximate the data X in two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using $Z = X * V$\n",
    "\n",
    "<img src=\"img/xv.png\" alt=\"XV\" style=\"width:700px;height:auto;\">\n",
    "\n",
    "Recall that the columns of Z are the latent features.\n",
    "\n",
    "The first column of Z is the latent feature with the largest variance,\n",
    "and the second column of Z is the latent feature with the second largest variance\n",
    "that is orthogonal to the first column.\n",
    "\n",
    "In this example, Z has the same dimensions as the first two columns of X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can construct Z using the V matrix (transpose Vt!)\n",
    "# The columns of V are the PCs, so the rows of Vt are the PCs.\n",
    "\n",
    "print('X (truncated):')\n",
    "print(X.head())\n",
    "print()\n",
    "\n",
    "print('Vt:')\n",
    "print(Vt)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Z using only the first two PCs\n",
    "Z = X.to_numpy() @ Vt.T[:,:2]\n",
    "pd.DataFrame(Z).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using $Z = U * S$\n",
    "\n",
    "Recall that $Z = XV = (USV^T)V = US$, since V is orthonormal.\n",
    "\n",
    "<img src=\"img/us.png\" alt=\"US\" style=\"width:700px;height:auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First two columns of U (truncated):')\n",
    "print(U[:10, :2])\n",
    "print()\n",
    "\n",
    "print('S:')\n",
    "np.diag(S[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct Z using the first two columns of U and the first two singular values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = U[:, :2] @ np.diag(S[:2])\n",
    "print(Z.shape)\n",
    "pd.DataFrame(Z).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of U are just the normalized (i.e., length 1) columns of Z:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize first column of Z using L2 norm\n",
    "length_of_col_1 = np.sqrt(np.sum(Z[:, 0]**2))\n",
    "normed_z = Z[:, 0] / length_of_col_1\n",
    "print(normed_z[:10])\n",
    "\n",
    "print(U[:10, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that the singular values are just the length of the column vectors of Z:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of first column of Z (L2 norm)\n",
    "print(np.sqrt(np.sum(Z[:, 0]**2)))\n",
    "\n",
    "# Identical to code above\n",
    "print(np.linalg.norm(Z[:, 0]))\n",
    "\n",
    "# Print first singular value\n",
    "print(S[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same results if we fit PCA with `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# This code computes first two columns of Z (i.e., the first two latent features)\n",
    "# And, yes, this whole lecture can be summarized by these two lines of code! \n",
    "\n",
    "# Initialize a PCA model object with 2 components\n",
    "pca = PCA(2)\n",
    "\n",
    "# Fit the PCA model to the data\n",
    "pd.DataFrame(pca.fit_transform(X)).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Z we computed is identical to the one from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Z).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the covariance matrix of Z is **diagonalized**, since the latent features are **uncorrelated**, unlike the original features.\n",
    "\n",
    "- In other words, the off-diagonal elements are 0 since the covariance between features is 0.\n",
    "\n",
    "- The diagonal elements are the variance of each latent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Covariance matrix of Z is diagonalized, since latent features are uncorrelated:')\n",
    "print(pd.DataFrame(np.cov(Z.T)))\n",
    "print()\n",
    "\n",
    "print('Covariance matrix of X is NOT diagonalized, since original features are correlated:')\n",
    "print(pd.DataFrame(np.cov(X.T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Rank Approximation of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to recover X from our approximation.\n",
    "\n",
    "In other words, we do the reverse transformation: Transform our latent features back\n",
    "to the original scale of the data, and then see how close we are to the\n",
    "original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectangle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use two principal components\n",
    "k = 2\n",
    "\n",
    "U, S, Vt = np.linalg.svd(X, full_matrices = False)\n",
    "\n",
    "## Construct the latent features\n",
    "Z = U[:,:k] @ np.diag(S[:k])\n",
    "\n",
    "## Approximate the original rectangle using the latent features Z and the principle components.\n",
    "# Remember that X = USVt = ZVt. If we only use the first two columns of U, \n",
    "# first two singular values, and first two principal components, this\n",
    "# equation becomes an approximation of the original data X. \n",
    "rectangle_hat = pd.DataFrame(Z @ Vt[:k, :], columns = rectangle.columns)\n",
    "\n",
    "## Scale and shift the factors back to the original coordinate system.\n",
    "# Recall that we standardized the original data by subtracting the mean\n",
    "# and dividing by the SD. We do this in reverse to get back to the natural scale.\n",
    "rectangle_hat = rectangle_hat * np.std(rectangle, axis = 0) + np.mean(rectangle, axis = 0)\n",
    "\n",
    "print(\"Shape of approximated data:\", rectangle_hat.shape)\n",
    "rectangle_hat.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the data\n",
    "fig = px.scatter_3d(rectangle, x=\"width\", y=\"height\", z=\"area\",\n",
    "                    width=800, height=600)\n",
    "fig.add_scatter3d(x=rectangle_hat[\"width\"], \n",
    "                  y=rectangle_hat[\"height\"], \n",
    "                  z=rectangle_hat[\"area\"], \n",
    "                  mode=\"markers\", name = \"approximation\")\n",
    "\n",
    "fig.update_layout(scene=dict(\n",
    "  xaxis=dict(title=dict(font=dict(size=22))),\n",
    "  yaxis=dict(title=dict(font=dict(size=22))),\n",
    "  zaxis=dict(title=dict(font=dict(size=22)))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "</br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "**Instructor Note: Return to Lecture!**\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congressional Vote Records\n",
    "\n",
    "Let's examine how the House of Representatives (of the 116th Congress, 1st session) voted in the month of **September 2019**.\n",
    "\n",
    "From the [U.S. Senate website](https://www.senate.gov/reference/Index/Votes.htm):\n",
    "\n",
    "> Roll call votes occur when a representative or senator votes \"yea\" or \"nay,\" so that the names of members voting on each side are recorded. A voice vote is a vote in which those in favor or against a measure say \"yea\" or \"nay,\" respectively, without the names or tallies of members voting on each side being recorded.\n",
    "\n",
    "The data, compiled from ProPublica [source](https://github.com/eyeseast/propublica-congress), is a \"skinny\" table of data where each record is a single vote by a member across any roll call in the 116th Congress, 1st session, as downloaded in February 2020. The member of the House, whom we'll call **legislator**, is denoted by their bioguide alphanumeric ID in http://bioguide.congress.gov/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# February 2019 House of Representatives roll call votes\n",
    "# Downloaded using https://github.com/eyeseast/propublica-congress\n",
    "votes = pd.read_csv('data/votes.csv')\n",
    "votes = votes.astype({\"roll call\": str}) \n",
    "votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we pivot this table to group each legislator and their voting pattern across every (roll call) vote in this month. We mark 1 if the legislator voted Yes (yea), and 0 otherwise (No/nay, no vote, speaker, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def was_yes(s):\n",
    "    return 1 if s.iloc[0] == \"Yes\" else 0    \n",
    "vote_pivot = votes.pivot_table(index='member', \n",
    "                                columns='roll call', \n",
    "                                values='vote', \n",
    "                                aggfunc=was_yes, \n",
    "                                fill_value=0)\n",
    "print(vote_pivot.shape)\n",
    "vote_pivot.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we analyze this data?\n",
    "\n",
    "While we could consider loading information about the legislator, such as their party, and see how this relates to their voting pattern, it turns out that we can do a lot with PCA to **cluster legislators by how they vote**.\n",
    "\n",
    "- You can also draw analogies to other kinds thumbs up / thumbs down scenarios, like Netflix. You can imagine the rows being customers, and the columns being the content they have clicked on or watched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to standardize/normalize since all of the columns are on the \n",
    "# same 0/1 scale, but we still center.\n",
    "vote_pivot_centered = vote_pivot - np.mean(vote_pivot, axis = 0)\n",
    "vote_pivot_centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 441 members of congress, 41 bills voted on\n",
    "vote_pivot_centered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the SVD of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vt = np.linalg.svd(vote_pivot_centered, full_matrices = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"u.shape\", u.shape)\n",
    "print(\"s.shape\", s.shape)\n",
    "print(\"vt.shape\", vt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_2d = pd.DataFrame(index = vote_pivot_centered.index)\n",
    "\n",
    "# Get 3 latent features by multiplying the first 3 columns of U and the first 3 colummns of S\n",
    "vote_2d[[\"z1\", \"z2\", \"z3\"]] = (u * s)[:, :3]\n",
    "\n",
    "# But, we will only plot the first two latent features\n",
    "fig = px.scatter(vote_2d, x='z1', y='z2', title='Vote Data', width=800, height=600, render_mode=\"svg\")\n",
    "\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=22))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be interesting to see the political affiliation for each vote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the first two singular values are large and all others are small, then two dimensions are enough to describe most of what distinguishes one observation from another. If not, then a PCA scatter plot is omitting lots of information.\n",
    "\n",
    "An equivalent way to evaluate this is to determine the **variance ratios**, i.e., the fraction of the variance each PC contributes to total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC1 explains 80% of the variance\n",
    "# PC2 explains 5% of the variance\n",
    "# PC3 explains 2% of the variance\n",
    "# and so on\n",
    "s**2 / sum(s**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of PCs is the same as the total number of bills voted on, so long as there are more congress members than bills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scree plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **scree plot** (and where its \"elbow\" is located) is a visual way of checking the distribution of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=range(1, len(s) + 1), y=s**2 / sum(s**2), title='Variance Explained', width=700, height=400, markers=True)\n",
    "fig.update_xaxes(title_text='Principal Component #')\n",
    "fig.update_yaxes(title_text='Proportion of Variance Explained')\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=16))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(vote_2d, x='z1', y='z2', z='z3', title='Vote Data', width=800, height=600)\n",
    "fig.update_traces(marker=dict(size=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baesd on the plot above, it looks like there are two clusters of datapoints. What do you think this corresponds to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating Member Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we load in more member information, from https://github.com/unitedstates/congress-legislators. This includes each legislator's political party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get current information about legislators with this code. In our case, we'll use\n",
    "# a static copy of the 2019 membership roster to properly match our voting data.\n",
    "\n",
    "# base_url = 'https://raw.githubusercontent.com/unitedstates/congress-legislators/main/'\n",
    "# legislators_path = 'legislators-current.yaml'\n",
    "# f = fetch_and_cache(base_url + legislators_path, legislators_path)\n",
    "\n",
    "# Use 2019 data copy\n",
    "legislators_data = yaml.safe_load(open('data/legislators-2019.yaml'))\n",
    "\n",
    "def to_date(s):\n",
    "    return datetime.strptime(s, '%Y-%m-%d')\n",
    "\n",
    "legs = pd.DataFrame(\n",
    "    columns=['leg_id', 'first', 'last', 'gender', 'state', 'chamber', 'party', 'birthday'],\n",
    "    data=[[x['id']['bioguide'], \n",
    "           x['name']['first'],\n",
    "           x['name']['last'],\n",
    "           x['bio']['gender'],\n",
    "           x['terms'][-1]['state'],\n",
    "           x['terms'][-1]['type'],\n",
    "           x['terms'][-1]['party'],\n",
    "           to_date(x['bio']['birthday'])] for x in legislators_data])\n",
    "legs['age'] = 2024 - legs['birthday'].dt.year\n",
    "legs.set_index(\"leg_id\")\n",
    "legs.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the vote data projected onto the principal components with the biographic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_2d = vote_2d.join(legs.set_index('leg_id')).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can visualize this data all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(vote_2d, x='z1', y='z2', color='party', symbol=\"gender\", size='age',\n",
    "           title='Vote Data', width=800, height=600, size_max=10,\n",
    "           opacity = 0.7,\n",
    "           color_discrete_map={'Democrat':'blue', 'Republican':'red', \"Independent\": \"green\"},\n",
    "           hover_data=['first', 'last', 'state', 'party', 'gender', 'age'],\n",
    "           render_mode=\"svg\")\n",
    "\n",
    "# Increase axis title size\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=22))\n",
    ")\n",
    "\n",
    "# Increase legend text size\n",
    "fig.update_layout(legend=dict(font=dict(size=16)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a bunch of overplotting, so let's jitter a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "vote_2d['z1_jittered'] = vote_2d['z1'] + np.random.normal(0, 0.1, len(vote_2d))\n",
    "vote_2d['z2_jittered'] = vote_2d['z2'] + np.random.normal(0, 0.1, len(vote_2d))\n",
    "vote_2d['z3_jittered'] = vote_2d['z3'] + np.random.normal(0, 0.1, len(vote_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(vote_2d, x='z1_jittered', y='z2_jittered', color='party', symbol=\"gender\", size='age',\n",
    "           title='Vote Data', width=800, height=600, size_max=10,\n",
    "           opacity = 0.7,\n",
    "           color_discrete_map={'Democrat':'blue', 'Republican':'red', \"Independent\": \"green\"},\n",
    "           hover_data=['first', 'last', 'state', 'party', 'gender', 'age'])\n",
    "\n",
    "# Increase axis title size\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=22))\n",
    ")\n",
    "\n",
    "# Increase legend text size\n",
    "fig.update_layout(legend=dict(font=dict(size=16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(\n",
    "    vote_2d, x='z1_jittered', y='z2_jittered', z='z3_jittered', \n",
    "    color='party', symbol=\"gender\", size='age',\n",
    "    title='Vote Data', width=800, height=600, size_max=10,\n",
    "    opacity = 0.7,\n",
    "    color_discrete_map={'Democrat':'blue', 'Republican':'red', \"Independent\": \"green\"},\n",
    "    hover_data=['first', 'last', 'state', 'party', 'gender', 'age']\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Analysis: Regular Voters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not everyone voted all the time.  Let's examine the frequency of voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's recompute the pivot table where we only consider Yes/No votes, and ignore records with \"No Vote\" or other entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_2d[\"num votes\"] = (\n",
    "    votes[votes[\"vote\"].isin([\"Yes\", \"No\"])]\n",
    "        .groupby(\"member\").size()\n",
    ")\n",
    "vote_2d.dropna(inplace=True)\n",
    "vote_2d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(vote_2d, x=\"num votes\", log_x=True, width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(vote_2d, x='z1_jittered', y='z2_jittered', color='party', symbol=\"gender\", size='num votes',\n",
    "           title='Vote Data (Size is Number of Votes)', width=800, height=600, size_max=10,\n",
    "           opacity = 0.7,\n",
    "           color_discrete_map={'Democrat':'blue', 'Republican':'red', \"Independent\": \"green\"},\n",
    "           hover_data=['first', 'last', 'state', 'party', 'gender', 'age'], \n",
    "           render_mode=\"svg\")\n",
    "\n",
    "# Change x axis title to PC1\n",
    "fig.update_xaxes(title_text='PC1')\n",
    "# Change y axis title to PC2\n",
    "fig.update_yaxes(title_text='PC2')\n",
    "# Increase axis title size\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=22)),\n",
    "  legend=dict(font=dict(size=16)),\n",
    ")\n",
    "# Move legend to bottom left of the plot\n",
    "fig.update_layout(legend=dict(\n",
    "  x=0.1,\n",
    "  y=0.1,\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at `Vt` directly to try to gain insight into why each component is as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PC1: How much does each bill contribute to the first latent feature?\n",
    "fig_eig = px.bar(x=vote_pivot_centered.columns, y=vt[0,:])\n",
    "fig_eig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the party affiliation labels so we can see if this eigenvector aligns with one of the parties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_line_votes = (\n",
    "    vote_pivot_centered.join(legs.set_index(\"leg_id\")['party'])\n",
    "                       .groupby(\"party\").mean()\n",
    "                       .T.reset_index()\n",
    "                       .rename(columns={\"index\": \"call\"})\n",
    "                       .melt(\"call\")\n",
    ")\n",
    "fig = px.bar(\n",
    "    party_line_votes,\n",
    "    x=\"call\", y=\"value\", facet_row = \"party\", color=\"party\",\n",
    "    color_discrete_map={'Democrat':'blue', 'Republican':'red', \"Independent\": \"green\"})\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like PC1 communicates something about voting Republican."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biplot\n",
    "\n",
    "To get a sense of how each of the features contributes to our PCs, we can plot \n",
    "the PC influence on our PCA plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First two rows of Vt give recipes for how much each vote contributes to the \n",
    "# first two latent features. Recipes are the PCs. \n",
    "directions = pd.DataFrame(\n",
    "    {\n",
    "    \"pc1\": vt[0,:], \n",
    "    \"pc2\": vt[1,:]\n",
    "    }, \n",
    "    index=vote_pivot_centered.columns)   \n",
    "directions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the rows above as 2D vectors. The rows tell us how much\n",
    "each bill contributes to the construction of each PC.\n",
    "\n",
    "**Be sure to zoom in to see the vectors at the center of the plot**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    vote_2d, x='z1_jittered', y='z2_jittered', color='party', symbol=\"gender\", size='num votes',\n",
    "    title='Biplot', width=800, height=600, size_max=10,\n",
    "    opacity = 0.7,\n",
    "    color_discrete_map={'Democrat':'blue', 'Republican':'red', \"Independent\": \"green\"},\n",
    "    hover_data=['first', 'last', 'state', 'party', 'gender', 'age'],\n",
    "    render_mode=\"svg\")\n",
    "\n",
    "for (call, pc1, pc2) in directions.head(50).itertuples():\n",
    "    fig.add_scatter(x=[0,pc1], y=[0,pc2], name=call, \n",
    "                    mode='lines+markers', textposition='top right',\n",
    "                    marker= dict(size=10,symbol= \"arrow-bar-up\", angleref=\"previous\"))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easier to see the PC influence on our biplot if we scale the vectors by the\n",
    "square root of the singular values ([reason out of scope](https://stats.stackexchange.com/questions/125684/how-does-fundamental-theorem-of-factor-analysis-apply-to-pca-or-how-are-pca-l))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recipe for how much each vote contributes to the first two latent\n",
    "# features.\n",
    "loadings = pd.DataFrame(\n",
    "    {\n",
    "    \"pc1\": np.sqrt(s[0]) * vt[0,:], \n",
    "    \"pc2\": np.sqrt(s[1]) * vt[1,:]\n",
    "    }, \n",
    "    index=vote_pivot_centered.columns)   \n",
    "loadings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "  vote_2d, x='z1_jittered', y='z2_jittered', color='party', symbol=\"gender\", size='num votes',\n",
    "  title='Biplot', width=800, height=600, size_max=10,\n",
    "  opacity = 0.7,\n",
    "  color_discrete_map={'Democrat':'blue', 'Republican':'red', \"Independent\": \"green\"},\n",
    "  hover_data=['first', 'last', 'state', 'party', 'gender', 'age'],\n",
    "  render_mode=\"svg\")\n",
    "\n",
    "\n",
    "for (call, pc1, pc2) in loadings.head(50).itertuples():\n",
    "    fig.add_scatter(x=[0,pc1], y=[0,pc2], name=call, \n",
    "                    mode='lines+markers', textposition='top right',\n",
    "                    marker= dict(size=10,symbol= \"arrow-bar-up\", angleref=\"previous\"))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "  xaxis_title=\"PC1\",\n",
    "  yaxis_title=\"PC2\",\n",
    "  xaxis=dict(title=dict(font=dict(size=22))),\n",
    "  yaxis=dict(title=dict(font=dict(size=22)))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each roll call from the 116th Congress - 1st Session: https://clerk.house.gov/evs/2019/ROLL_500.asp\n",
    "* 555: Raising a question of the privileges of the House ([H.Res.590](https://www.congress.gov/bill/116th-congress/house-resolution/590))\n",
    "* 553: [https://www.congress.gov/bill/116th-congress/senate-joint-resolution/54/actions]\n",
    "* 527: On Agreeing to the Amendment [H.R.1146 - Arctic Cultural and Coastal Plain Protection Act](https://www.congress.gov/bill/116th-congress/house-bill/1146)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion-MNIST dataset\n",
    "\n",
    "We will be using the Fashion-MNIST dataset, which is a cool little dataset with gray scale 28x28 images of articles of clothing.\n",
    "\n",
    "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. Han Xiao, Kashif Rasul, Roland Vollgraf. arXiv:1708.07747\n",
    "https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "print(\"Training images\", train_images.shape)\n",
    "print(\"Test images\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class names for this data are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "class_dict = {i:class_name for i, class_name in enumerate(class_names)}\n",
    "class_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this demo, let's take a small sample of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "n = 5000\n",
    "sample_idx = rng.choice(np.arange(len(train_images)), size=n, replace=False)\n",
    "\n",
    "# Invert and normalize the images so they look better\n",
    "img_mat = -1. * train_images[sample_idx]\n",
    "img_mat = (img_mat - img_mat.min())/(img_mat.max() - img_mat.min())\n",
    "\n",
    "images = pd.DataFrame({\"images\": img_mat.tolist(), \n",
    "                   \"labels\": train_labels[sample_idx], \n",
    "                   \"class\": [class_dict[x] for x in train_labels[sample_idx]]})\n",
    "images.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet of code visualizes the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, ncols=5, max_images=30):\n",
    "    # conver the subset of images into a n,28,28 matrix for facet visualization\n",
    "    img_mat = np.array(images.head(max_images)['images'].to_list())\n",
    "    fig = px.imshow(img_mat, color_continuous_scale='gray', \n",
    "                    facet_col = 0, facet_col_wrap=ncols,\n",
    "                    height = 220*int(np.ceil(len(images)/ncols)))\n",
    "    fig.update_layout(coloraxis_showscale=False)\n",
    "    # Extract the facet number and convert it back to the class label.\n",
    "    fig.for_each_annotation(lambda a: a.update(text=images.iloc[int(a.text.split(\"=\")[-1])]['class']))\n",
    "    return fig\n",
    "\n",
    "show_images(images.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(images.groupby('class',as_index=False).sample(2), ncols=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "How would we visualize the entire dataset?  Let's use PCA to find a low dimensional representation of the images. \n",
    "\n",
    "First, let's understand the high-dimensional representation. We will extract the matrix of images from the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(images['images'].to_list())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now \"unroll\" the pixels into a single row vector 28*28 = 784 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], -1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Center the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X - X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run PCA (this time we use `sklearn`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "n_comps = 50 \n",
    "pca = PCA(n_components=n_comps)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining PCA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a line plot and show markers\n",
    "px.line(y=pca.explained_variance_ratio_ *100, markers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of data is explained in first two or three dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[['z1', 'z2', 'z3']] = pca.transform(X)[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(images, x='z1', y='z2', hover_data=['labels'], opacity=0.7,\n",
    "           width = 800, height = 600, render_mode=\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA discovered the labels -- We never told PCA about dresses, coats, etc!\n",
    "# This is a nice illustration why PCA is useful in clustering.\n",
    "# It can help us find the \"natural\" clusters in high-dimensional data.\n",
    "px.scatter(images, x='z1', y='z2', color='class', hover_data=['labels'], opacity=0.7, \n",
    "           width = 800, height = 600, render_mode=\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(images, x='z1', y='z2', z='z3', color='class', hover_data=['labels'], \n",
    "                    width=1000, height=600)\n",
    "# set marker size to 5\n",
    "fig.update_traces(marker=dict(size=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
