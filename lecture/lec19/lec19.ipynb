{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d30bb2-585a-43bc-8306-cc6ed9cfb626",
   "metadata": {},
   "source": [
    "# ü•æ Lecture 19 ‚Äì Data 100, Spring 2025\n",
    "\n",
    "Data 100, Spring 2025\n",
    "\n",
    "[Acknowledgments Page](https://ds100.org/sp25/acks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import sklearn.linear_model as lm\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af3d30",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "# ‚è™ Data 8 Review: Bootstrapping, Confidence Intervals, and Hypothesis Testing\n",
    "\n",
    "Suppose we want to estimate some fixed population-level quantity, like the true average height of all 32,000 UC Berkeley undergraduates. We assume that this true average height is a **fixed but unknown** quantity. In other words, the population-level statistic is **not a random variable**.\n",
    "\n",
    "In a perfect world, we would measure the height of every UC Berkeley undergraduate, calculate the average height, and have a perfect answer to our question.\n",
    "But, cannot reasonably measure the height of every UC Berkeley undergraduate. \n",
    "Instead, we might take a random sample of, say, 10 UC Berkeley students, calculate the sample average, and then use that sample average as our \"best guess\" of the true average height of all 32,000 UC Berkeley students.\n",
    "\n",
    "Here's a (fake) random sample of 10 UC Berkeley undergraduate heights in inches, along with the sample mean of those heights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = np.array([68, 67, 69, 66, 66, 66, 71, 72, 61, 70])\n",
    "\n",
    "heights.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f512e7",
   "metadata": {},
   "source": [
    "We would say that 67.6 inches is our \"best guess\" of the true average height of UC Berkeley undergraduates.\n",
    "\n",
    "Unlike the true average height, your \"best guess\" is a **random** quantity. \n",
    "For example, if you and a friend separately sampled 10 UC Berkeley undergraduates, your sample average heights will probably differ due to randomness in the sample.\n",
    "This begs the question: How much could our \"best guesses\" differ?\n",
    "\n",
    "To answer this question, it would be useful for us to measure the **variability** of our sample statistic across **parallel universes** of random samples.\n",
    "But, we have another problem: We only get to observe one universe!\n",
    "\n",
    "In Data 8, you learned that bootstrapping can be used to construct **synthetic parallel universes**. For example, here's how we could construct synthetic bags of M&Ms from one bag of M&Ms:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/mms.png\" alt=\"m&ms and the bootstrap\" width=\"600\">\n",
    "</p>\n",
    "\n",
    "Here are the steps of bootstrapping written out:\n",
    "\n",
    "0. Assume that your random sample of size `n` is representative of the true population.\n",
    "1. To mimic a random draw of size `n` from the true population, randomly resample `n` observations **with replacement** from your random sample. Call this a \"synthetic\" random sample.\n",
    "2. To compute a synthetic \"best guess\", caculate the sample statistic using your synthetic random sample. For example, you could calculate the sample average.\n",
    "3. Repeat steps 1 and 2 many times. A common choice is 10,000 times.\n",
    "4. The distribution of the 10,000 synthetic \"best guesses\" provide a sense of uncertainty around your original \"best guess\".\n",
    "\n",
    "Here's how we could generate just one synthetic random sample of heights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(100)\n",
    "\n",
    "sample_size = len(heights)\n",
    "\n",
    "# Resample n values with replacement from our real sample of 10 heights\n",
    "synth_heights = np.random.choice(heights, size=sample_size, replace=True)\n",
    "\n",
    "# Compute the mean of the synthetic sample\n",
    "synth_estimate = synth_heights.mean()\n",
    "\n",
    "print(\"Original Heights:\")\n",
    "print(heights)\n",
    "print(\"Mean of Original Heights:\")\n",
    "print(heights.mean())\n",
    "print()\n",
    "print(\"Synthetic Heights:\")\n",
    "print(synth_heights)\n",
    "print(\"Mean of Synthetic Heights:\")\n",
    "print(synth_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584d1db",
   "metadata": {},
   "source": [
    "Notice that our synthetic sample mean of 67 inches differs from our original \"best guess\" of 67.6 inches.\n",
    "\n",
    "Let's repeat this 10,000 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a94f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = len(heights)\n",
    "n_boot = 10000\n",
    "\n",
    "# Create an empty array to hold the 10,000 synthetic \"best guesses\"\n",
    "synth_estimates = np.zeros(n_boot)\n",
    "\n",
    "for i in range(n_boot):\n",
    "\n",
    "  # Resample n values with replacement from our real sample of 10 heights\n",
    "  synth_heights = np.random.choice(heights, size=sample_size, replace=True)\n",
    "\n",
    "  # Compute the mean of the synthetic sample\n",
    "  synth_estimate = synth_heights.mean()\n",
    "\n",
    "  # Append the synthetic mean to synth_estimates\n",
    "  synth_estimates[i] = synth_estimate\n",
    "\n",
    "print('Number of synthetic best guesses:')\n",
    "print(len(synth_estimates))\n",
    "\n",
    "print('First 5 synthetic best guesses:')\n",
    "print(synth_estimates[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ac4e66",
   "metadata": {},
   "source": [
    "To get a sense of how much our best guess could vary across parallel universes, we can visualize the resulting distribution of synthetic best guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(pd.Series(synth_estimates), \n",
    "            title='Bootstrap Distribution of the Sample Mean Height', \n",
    "            width=800, histnorm='probability', \n",
    "            barmode=\"overlay\", opacity=0.8)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ff5aa",
   "metadata": {},
   "source": [
    "It looks like the best guesses of most parallel universes fall between 65 inches and 70 inches.\n",
    "\n",
    "### Bootstrap confidence intervals\n",
    "\n",
    "Suppose a construction manager at Berkeley asked you to find the sample mean height of Berkeley undergraduates so that doors in a new building were not too high or too low.\n",
    "It would be a good idea to **not only** provide your best guess of 67 inches, but also provide a sense of how **uncertain** you are about your best guess.\n",
    "\n",
    "To do so, we can construct and report a bootstrap **confidence interval (CI)**.\n",
    "For example, to construct a 95\\% CI, we would grab the middle 95\\% of the synthetic best guesses.\n",
    "In other words, we would grab the 2.5th percentile and the 97.5th percentile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the 2.5th and 97.5th percentiles of the synthetic sample means\n",
    "ci_bounds = np.percentile(synth_estimates, [2.5, 97.5])\n",
    "\n",
    "print(\"Lower bound of 95% CI: \", ci_bounds[0])\n",
    "\n",
    "print(\"Upper bound of 95% CI: \", ci_bounds[1])\n",
    "\n",
    "fig.add_vline(x=ci_bounds[0], line_color='red')\n",
    "fig.add_vline(x=ci_bounds[1], line_color='red')\n",
    "fig.add_annotation(x=ci_bounds[0], y=0.02, text=\"Lower Bound\", showarrow=True, arrowhead=2)\n",
    "fig.add_annotation(x=ci_bounds[1], y=0.02, text=\"Upper Bound\", showarrow=True, arrowhead=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea5eaf",
   "metadata": {},
   "source": [
    "Given the values above, we could say **\"We are 95\\% confident that the true average height of UC Berkeley undergrads is between 65.6 inches and 69.4 inches.\"**\n",
    "\n",
    "- What \"confidence\" really means: Across parallel universes, we think that 95\\% of our synthetic sample estimates would fall between 65.6 inches and 69.4 inches.\n",
    "\n",
    "This knowledge of uncertainty around our best guess can help us make better decisions than a single best guess alone.\n",
    "\n",
    "### Bootstrap hypothesis testing\n",
    "\n",
    "We can also use our confidence interval to perform **hypothesis testing**. \n",
    "For example, someone might claim that the true average height of UC Berkeley undergrads is 68 inches.\n",
    "This person has proposed what is called a **null hypothesis.**\n",
    "\n",
    "$$\n",
    "\\text{Null hypothesis }H_0: \\mu = 68,\n",
    "$$\n",
    "\n",
    "In the definition above, $\\mu$ is claimed the true average height of Berkeley undergrads. We can show this null population mean on our plot from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4addd5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.add_vline(x=68, line_color='green')\n",
    "fig.add_annotation(x=68, y=0.02, text=\"Null population mean\", showarrow=True, arrowhead=2)\n",
    "fig.add_annotation(x=ci_bounds[1], y=0.02, text=\"Upper Bound\", showarrow=True, arrowhead=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b788a388",
   "metadata": {},
   "source": [
    "\n",
    "Our best guess of the true average height based on our original sample is **67.6 inches**. But, our bootstrap distribution of the sample mean shows that we could have **plausibly made a best guess of 68 inches**, in a parallel universe.\n",
    "\n",
    "Statistical logic (out of scope) tells us to therefore **fail to reject** the hypothesis that the true average height is 68 inches at a **5\\% significance level**.\n",
    "\n",
    "- This does not mean that the true average height is exactly 68 inches (i.e., that the hypothesis is true, or that we accept the hypothesis).\n",
    "- Instead, all we can say is that our sample data could have plausibly been observed if the true average height were indeed 68 inches, so we can't rule out the possibility that the true average height is in fact 68 inches. \n",
    "- The significance level is calculated by subtracting the confidence level from 1.\n",
    "\n",
    "If we claimed that the true average height were, say, 70 inches, then we would **reject the null hypothesis** at the 5\\% level, since 70 inches falls outside of our 95\\% confidence interval.\n",
    "\n",
    "- In other words, it is unlikely that we could have observed a sample average height of 70 inches in a parallel universe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a98e5f3",
   "metadata": {},
   "source": [
    "### From last lecture: Bias, variance, and MSE of an estimator\n",
    "\n",
    "In the last lecture, we learned about the bias, variance, and MSE of an estimator, which are composed of expectations over infinite possible random samples of the data:\n",
    "\n",
    "$$\n",
    "\\text{Bias}(\\hat{\\theta}) = \\mathbb{E} \\left[ \\hat{\\theta} \\right] - \\theta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Variance}(\\hat{\\theta}) = \\mathbb{E}\\left[ \\left( \\hat{\\theta} - \\mathbb{E}(\\hat{\\theta}) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{MSE}(\\hat{\\theta}) = \\mathbb{E}\\left[ \\left( \\hat{\\theta} - \\theta \\right)^2 \\right] \n",
    "$$\n",
    "\n",
    "Notice that the variance formula does not require us to know $\\theta$. \n",
    "So, we can estimate the variance of our estimator using the bootstrap distribution of $\\hat{\\theta}$'s.\n",
    "\n",
    "> Estimating variance is an important component of constructing normally-approximated bootstrapped confidence intervals, which are beyond the scope of Data 100.\n",
    "\n",
    "To estimate **expectation** from a random sample of `B` synthetic values of $\\hat{\\theta}$, we just compute the **sample mean** of the $\\hat{\\theta}$'s:\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbb{E}(\\hat{\\theta})} = \\text{Sample mean of }\\hat{\\theta}\\text{'s} = \\bar{\\hat{\\theta}} = \\frac{1}{B} \\sum_{i=1}^B \\hat{\\theta}_i\n",
    "$$\n",
    "\n",
    "To estimate **variance** from a random sample of `B` synthetic values of $\\hat{\\theta}$, we just compute the **sample variance** of the $\\hat{\\theta}$'s:\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{Var}(\\hat{\\theta})} = \\text{Sample variance of }\\hat{\\theta}\\text{'s} \\approx \\frac{1}{B} \\sum_{i=1}^B (\\hat{\\theta}_i - \\bar{\\hat{\\theta}} )^2\n",
    "$$\n",
    "\n",
    "> The $\\approx$ above is out of scope for Data 100. Don't worry about it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate expectation using a mean:\n",
    "estimated_expectation_theta_hat = synth_estimates.mean()\n",
    "\n",
    "# Same idea for estimated variance\n",
    "estimated_variance_theta_hat = np.mean((synth_estimates - estimated_expectation_theta_hat) ** 2)\n",
    "\n",
    "print(\"Estimated variance of the synthetic sample means: \", estimated_variance_theta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a768a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.var directly computes the variance of an array, so we get the same answer!\n",
    "sample_variance_est = np.var(synth_estimates)\n",
    "print(\"Estimated variance of the synthetic sample means: \", sample_variance_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7930ad7",
   "metadata": {},
   "source": [
    "Important Data 100 skill from this section: Understand how population level quantities like $\\mathbb{E}(\\text{Something})$ and $\\text{Var}(\\text{Something})$ can be estimated with a sample of $\\text{Something}_i$'s.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Instructor note: Return to slides!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fffa7a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üçù Bootstrapping a regression coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c00524f",
   "metadata": {},
   "source": [
    "For demonstration, we will fit an SLR model to a random sample of the `mpg` dataset predicting `mpg` (miles per gallon) from the `weight` (weight of the vehicle):\n",
    "\n",
    "$$ \n",
    "\\widehat{\\text{mpg}} = \\hat{\\theta}_0 + \\hat{\\theta}_1 * \\text{weight}\n",
    "$$\n",
    "\n",
    "Then, using the bootstrap, we will construct a confidence interval around the $\\hat{\\theta}_1$ coefficient.\n",
    "\n",
    "- This confidence interval tells us the values of $\\hat{\\theta}_1$ that we could have observed in a parallel universe where a different random sample of cars of the same size were selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21369b",
   "metadata": {},
   "source": [
    "Suppose we collected a simple random sample of 20 cars from a population of cars.  For the purposes of this demo we will assume that `seaborn`'s `mpg` dataset is the population of all cars. \n",
    "\n",
    "Here's a visualization of our sample and SLR model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebebf2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of cars to sample\n",
    "sample_size = 20\n",
    "\n",
    "# Load in the mpg from seaborn\n",
    "mpg = sns.load_dataset('mpg')\n",
    "print(\"Full Data Size:\", len(mpg))\n",
    "\n",
    "# Sample `sample_size` rows from the mpg dataset\n",
    "mpg_sample = mpg.sample(sample_size)\n",
    "print(\"Sample Size:\", len(mpg_sample))\n",
    "\n",
    "px.scatter(mpg_sample, x='weight', y='mpg', trendline='ols', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b76a7ff",
   "metadata": {},
   "source": [
    "We can fit linear model with `sklearn` to get an estimate of the slope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ccca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lm.LinearRegression().fit(mpg_sample[['weight']], mpg_sample['mpg'])\n",
    "\n",
    "print(\"Slope of the regression line: \", model.coef_[0])\n",
    "print(\"Intercept of the regression line: \", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0906ab",
   "metadata": {},
   "source": [
    "Our \"best guess\" of the estimated increase in `mpg` associated with a one-unit increase in `weight` is -0.007.\n",
    "\n",
    "But, best guess is not the end of the story! We can use the bootstrap to measure **uncertainty** around our best guess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc106b0",
   "metadata": {},
   "source": [
    "### Bootstrap Implementation\n",
    "\n",
    "Now let's use the bootstrap to estimate what $\\hat{\\theta}_1$ might look like across parallel universes of SLR models fit to different random samples.\n",
    "\n",
    "To make our code reusable, let's write a `bootstrap` function that takes in a random sample and an estimator function (i.e., the sample mean or a function to extract the $\\hat{\\theta}_1$ coefficient from an SLR), and then uses that estimator function to construct many synthetic estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e14266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator(sample):\n",
    "    \"\"\"\n",
    "    Fits an SLR to `sample` regressing mpg on weight, \n",
    "    and returns the slope of the fitted line\n",
    "    \"\"\"\n",
    "    model = lm.LinearRegression().fit(sample[['weight']], sample['mpg'])\n",
    "\n",
    "    return (model.intercept_, model.coef_[0])\n",
    "\n",
    "estimator(mpg_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745b733a",
   "metadata": {},
   "source": [
    "As expected, our estimator function returns the same intercept and slope as the model we fit above, so long as we plug in the original sample of cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aa38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Slope of the regression line: \", model.coef_[0])\n",
    "print(\"Intercept of the regression line: \", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75387946",
   "metadata": {},
   "source": [
    "Next, we will write the general-purpose `bootstrap` function.\n",
    "\n",
    "> Writing a general-purpose bootstrap function is a very classic problem. It could\n",
    "> show up in an interview or on-the-job. Good to understand this section well!\n",
    "\n",
    " The `bootstrap` function code uses `df.sample` ([link](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html)) to generate a bootstrap sample of the same size of the original sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(sample, statistic, num_repetitions):\n",
    "    \"\"\"\n",
    "    Returns the statistic computed on a num_repetitions  \n",
    "    bootstrap samples from sample.\n",
    "    \"\"\"\n",
    "    stats = []\n",
    "\n",
    "    # tqdm provides a progress bar\n",
    "    # functionally, this code is the same as `for i in np.arange(num_repetitions)`\n",
    "    for i in tqdm(np.arange(num_repetitions), \"Bootstrapping\"):\n",
    "        \n",
    "        # Step 1: Resample with replacement from our original sample to generate\n",
    "        # a synthetic sample of the same size\n",
    "        bootstrap_sample = sample.sample(frac=1, replace=True)\n",
    "\n",
    "        # Step 2: Calculate a synthetic estimate using the synthetic sample\n",
    "        bootstrap_stat = statistic(bootstrap_sample)\n",
    "\n",
    "        # Append the synthetic estimate to the list of estimates\n",
    "        stats.append(bootstrap_stat)\n",
    "        \n",
    "    return stats    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f25d9f",
   "metadata": {},
   "source": [
    "Constructing MANY bootstrap slope estimates:\n",
    "\n",
    "> In general, `10,000` is a good default for the number of synthetic samples to compute.\n",
    "> In this case, we will use `1,000` just so our code runs a little faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3782ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_thetas = bootstrap(mpg_sample, estimator, 1000)\n",
    "print(\"Number of bootstrap estimates:\", len(bs_thetas))\n",
    "print(\"First 5 bootstrap estimates:\", bs_thetas[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20d1e8c",
   "metadata": {},
   "source": [
    "We can visualize the 1,000 synthetic SLR models we fit with the bootstrap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a170aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the SLR models given in bs_thetas\n",
    "# Create a DataFrame from the list of tuples\n",
    "\n",
    "# Make a scatterplot of the original data\n",
    "fig = px.scatter(mpg_sample, x='weight', y='mpg', trendline='ols', width=800)\n",
    "\n",
    "for theta in bs_thetas:\n",
    "    \n",
    "    # Unpack the tuple\n",
    "    intercept, slope = theta\n",
    "\n",
    "    # Create a line from the intercept and slope\n",
    "    x = np.linspace(1500, 5000, 100)\n",
    "    y = intercept + slope * x\n",
    "\n",
    "    # Plot the lines transparently\n",
    "    fig.add_scatter(x=x, y=y, mode='lines', line=dict(width=0.05))\n",
    "\n",
    "fig.update_layout(title='Bootstrapped SLR Models')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d25930",
   "metadata": {},
   "source": [
    "From the plot above, can you guess why the emoji for this section is spaghetti üçù?\n",
    "\n",
    "We were originally just interested in conducting inference on the slope, $\\hat{\\theta}_1$. \n",
    "So, let's visualize the bootstrap distribution of the synthetic $\\hat{\\theta}_1$ estimates:\n",
    "\n",
    "> Note we could have done the same for the synthetic $\\hat{\\theta}_0$ estimates, too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e13010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the slopes from the list of (intercept, slope) tuples\n",
    "bs_theta1s = [theta[1] for theta in bs_thetas]\n",
    "\n",
    "fig = px.histogram(pd.Series(bs_theta1s),\n",
    "                   title='Bootstrap Distribution of the Slope', \n",
    "                   width=800, histnorm='probability', \n",
    "                   barmode=\"overlay\", opacity=0.8)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784bae3",
   "metadata": {},
   "source": [
    "### Computing a Bootstrap CI\n",
    "We can compute the CI using the percentiles of the distribution of 1,000 synthetic estimates of $\\hat{\\theta}_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(bootstrap_estimates, confidence_level=95):\n",
    "    \"\"\"\n",
    "    Returns the confidence interval for the synthetic estimates by grabbing\n",
    "    the percentiles corresponding to `confidence_level`% of the samples\n",
    "    \"\"\"\n",
    "    lower_percentile = (100 - confidence_level) / 2\n",
    "    upper_percentile = 100 - lower_percentile\n",
    "\n",
    "    # np.percentile grabs the given percentiles of an array\n",
    "    return np.percentile(bootstrap_estimates, [lower_percentile, upper_percentile])\n",
    "\n",
    "print(bootstrap_ci(bs_theta1s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e4331",
   "metadata": {},
   "source": [
    "Visualizing our resulting 95\\% confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f160cf-0d37-4080-90a9-05064b983fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_line_style = dict(color=\"orange\", width=2, dash=\"dash\")\n",
    "fig.add_vline(x=bootstrap_ci(bs_theta1s)[0], line=ci_line_style)\n",
    "fig.add_vline(x=bootstrap_ci(bs_theta1s)[1], line=ci_line_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3416b",
   "metadata": {},
   "source": [
    "Given the above, we can say that **\"We are 95\\% confident that the true $\\theta_1$ falls between -0.009 and -0.0055\".**\n",
    "\n",
    "- The true $\\theta_1$ would be obtained if we fit our SLR model to the entire population.\n",
    "\n",
    "Very often, we want to test whether a **regression coefficient is significantly different than 0**.\n",
    "\n",
    "- 0 is not contained in the interval above, so we can **reject the null hypothesis** that $\\theta_1=0$ at a 5% significance level.\n",
    "\n",
    "- In other words, it is highly unlikely that we would observe our actual sample data in a world where $\\theta_1=0$, simply due to randomness in the sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76152df2",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "**Instructor Note: Return to Lecture.**\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47099eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üü™ PurpleAir\n",
    "\n",
    "This example is from the Data 100 textbook: [link](https://learningds.org/ch/17/inf_pred_gen_boot.html). \n",
    " \n",
    "The following cell does some basic data cleaning. Don't worry about the details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e97aa4-fa89-4073-be8d-68a4397ac0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'data/Full24hrdataset.csv.gz'\n",
    "usecols = ['Date', 'ID', 'region', 'PM25FM', 'PM25cf1', 'TempC', 'RH', 'Dewpoint']\n",
    "full_df = pd.read_csv(csv_file, usecols=usecols, parse_dates=['Date']).dropna()\n",
    "full_df.columns = ['date', 'id', 'region', 'pm25aqs', 'pm25pa', 'temp', 'rh', 'dew']\n",
    "full_df = full_df[(full_df['pm25aqs'] < 50)]\n",
    "# drop dates with issues in the data\n",
    "bad_dates = pd.to_datetime(['2019-08-21', '2019-08-22', '2019-09-24'])\n",
    "\n",
    "# GA is the DataFrame that contains air quality measurements\n",
    "GA = full_df[(full_df['id'] == 'GA1') & (~full_df['date'].isin(bad_dates))]\n",
    "GA = GA.sort_values(\"pm25aqs\")\n",
    "display(full_df[\"region\"].value_counts())\n",
    "display(GA.head())\n",
    "print(\"Number of Rows:\", GA.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d4fe3e",
   "metadata": {},
   "source": [
    "### Inverse Regression\n",
    "\n",
    "After we build the model that adjusts the PurpleAir measurements using AQS, we then flip the model around and use it to predict the true air quality in the future from PurpleAir measurements when we don't have a nearby AQS instrument. \n",
    "This is a *calibration* scenario.\n",
    "Since the AQS measurements are close to the truth, we fit the more variable PurpleAir measurements to them;\n",
    "this is the calibration procedure. \n",
    "Then, we use the calibration curve to correct future PurpleAir measurements. \n",
    "This two-step process is encapsulated in the simple linear model and its flipped form below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facc585d-78d2-4659-89a0-5c65cbbc7b17",
   "metadata": {},
   "source": [
    "Inverse regression:\n",
    "* First, we **fit** a line to predict a PA measurement from the ground truth, as recorded by an AQS instrument:\n",
    "\n",
    "    $$ \\text{PA} \\approx \\theta_0 + \\theta_1\\text{AQS} $$\n",
    "\n",
    "* Next, we **invert the line** (i.e, we don't fit another model!) so we can use a PA measurement to predict the true air quality in places where AQS sensors are not available,\n",
    "\n",
    "    $$ \\text{True Air Quality} \\approx -\\theta_0/\\theta_1 + 1/\\theta_1 \\text{PA} $$\n",
    "    \n",
    "Why perform this ‚Äúinverse regression‚Äù?\n",
    "* Intuitively, AQS measurements are ‚Äútrue‚Äù and have no error.\n",
    "* A linear model assumes that the inputs and **fixed and known** and not random. We treat the PA estimates as **noisy and random**, and the AQS measures as **fixed and known** (i.e., accurate!).\n",
    "* Algebraically identical, but statistically different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e49f52-83ee-47fe-b61a-a71a463ef89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an SLR predicting purple air measurements from AQS measurements\n",
    "model = lm.LinearRegression().fit(GA[['pm25aqs']], GA['pm25pa'])\n",
    "theta_0, theta_1 = model.intercept_, model.coef_[0], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f00c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pm25 is a measure of air quality. pm stands for \"particulate matter\".\n",
    "fig = px.scatter(GA, x='pm25aqs', y='pm25pa', width=800)\n",
    "\n",
    "# This code adds the SLR fit to the scatterplot. Don't worry about the details of this code.\n",
    "xtest = pd.DataFrame({\"pm25aqs\": np.array([GA['pm25aqs'].min(), GA['pm25aqs'].max()])})\n",
    "fig.add_scatter(x=xtest[\"pm25aqs\"], y=model.predict(xtest[[\"pm25aqs\"]]), mode='lines', \n",
    "                name=\"Least Squares Fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c92b24",
   "metadata": {},
   "source": [
    "Invert the model by isolating the AQS term, and then change the name of the AQS term the \"true air quality estimate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e703a-41de-4310-a7ee-a11688453e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"True Air Quality Estimate = {-theta_0/theta_1:.2} + {1/theta_1:.2}PA\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54732320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code adds the inverse fit to the scatterplot. \n",
    "# It may look like we are fitting a new model, but we are just creating a model\n",
    "# object to make it easier to plot the inverse fit.\n",
    "# Don't worry about the details of this code.\n",
    "fig = px.scatter(GA, y='pm25aqs', x='pm25pa', width=800)\n",
    "model2 = lm.LinearRegression().fit(GA[['pm25pa']], GA['pm25aqs'])\n",
    "xtest[\"pm25pa\"] = np.array([GA['pm25pa'].min(), GA['pm25pa'].max()])\n",
    "fig.add_scatter(x=xtest[\"pm25pa\"], y=xtest[\"pm25pa\"] *1/theta_1 - theta_0/theta_1 , mode='lines', \n",
    "                name=\"Inverse Fit\")\n",
    "fig.add_scatter(x=xtest[\"pm25pa\"], y=model2.predict(xtest[['pm25pa']]), mode='lines',\n",
    "                name=\"Least Squares Fit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5946995-677c-45c2-a441-3681ec37a297",
   "metadata": {},
   "source": [
    "\n",
    "### The Barkjohn et al. model with Relative Humidity\n",
    "\n",
    "[Karoline Barkjohn, Brett Gannt, and Andrea Clements](https://amt.copernicus.org/articles/14/4617/2021/) from the US Environmental Protection Agency developed a model to improve the PuprleAir measurements from the AQS sensor measurements. Arkjohn and group‚Äôs work was so successful that, as of this writing, the official US government maps, like the [AirNow Fire and Smoke](https://fire.airnow.gov/) map, includes both AQS and PurpleAir sensors, and applies Barkjohn‚Äôs correction to the PurpleAir data.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{PA} \\approx \\theta_0 + \\theta_1 \\text{AQS} + \\theta_2 \\text{RH}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The model that Barkjohn settled on incorporates the **relative humidity**.\n",
    "\n",
    "- The code below fits and inverts the Barkjohn model to the data exactly as we did above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fba655-17f0-4017-928d-6364f8da6f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h = lm.LinearRegression().fit(GA[['pm25aqs', 'rh']], GA['pm25pa'])\n",
    "[theta_1, theta_2], theta_0 = model_h.coef_, model_h.intercept_\n",
    "\n",
    "print(f\"True Air Quality Estimate = {-theta_0/theta_1:1.2} + {1/theta_1:.2}PA + {-theta_2/theta_1:.2}RH\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3901a7",
   "metadata": {},
   "source": [
    "For comparison, here are the original coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"True Air Quality Estimate = {-theta_0/theta_1:.2} + {1/theta_1:.2}PA\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8da7e",
   "metadata": {},
   "source": [
    "Note that the coefficients on `PA` are similar, but the intercepts are quite different due to the inclusion of relative humidity.\n",
    "\n",
    "- The intercept represents the predicted air quality when `PA` and `RH` are 0, which is a different than the interpretation of the original intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cdace7-7014-44b7-ae4f-6eb5ca5d8212",
   "metadata": {},
   "source": [
    "<br/>Compared to the simple linear model that only incorporated AQS, the Barkjohn et al. model with relative humidity achieves lower error. Good for prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57285193-7ea3-4966-a67a-f4ab131c10bf",
   "metadata": {},
   "source": [
    "<br><br> \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Bootstrapping the regression coefficients for Purple Air"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c150c0c-6279-402d-8889-976f572ad1da",
   "metadata": {},
   "source": [
    "From the Barkjohn et al., model, AQS coefficient $\\hat{\\theta}_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae6976-7fde-46a0-90ae-47729549bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342c7f2-c608-4abe-bccc-05773b764f04",
   "metadata": {},
   "source": [
    "The Relative Humidity coefficient $\\hat{\\theta}_2$ is pretty close to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7623bdb-d47d-423f-b193-24eb45edbeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f31e52-00a7-47c6-ad83-943c108360a9",
   "metadata": {},
   "source": [
    "Is incorporating humidity in the model really needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc24e4-23b9-4184-85c6-d8c919271998",
   "metadata": {},
   "source": [
    "**Null hypothesis**: The null hypothesis is $\\theta_2 = 0$; that is, the null model is the simpler model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{PA} \\approx \\theta_0 + \\theta_1 \\text{AQS}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a99bbf-9e7b-440b-8f3a-e6a433b42731",
   "metadata": {},
   "source": [
    "Repeat 1,000 times to get an approximation to the boostrap sampling distirbution of the bootstrap statistic (the fitted humidity coefficient $\\hat{\\theta_2}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta2_estimate(sample):\n",
    "    model = lm.LinearRegression().fit(sample[['pm25aqs', 'rh']], sample['pm25pa'])\n",
    "    return model.coef_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf8d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_theta2 = bootstrap(GA, theta2_estimate, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3c762-e8ef-40b0-9266-1d6d75810801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.histogram(x=bs_theta2,\n",
    "                   labels=dict(x='Bootstrapped Humidity Coefficient'),\n",
    "                   histnorm='probability', \n",
    "                   width=800)\n",
    "fig.add_vline(0)\n",
    "fig.add_vline(x=bootstrap_ci(bs_theta2)[0], line=ci_line_style)\n",
    "fig.add_vline(x=bootstrap_ci(bs_theta2)[1], line=ci_line_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c010f-8365-4969-8493-ecdb713dfb33",
   "metadata": {},
   "source": [
    "(We know that the center will be close to the original coefficient estimated from the sample, 0.21.)\n",
    "\n",
    "By design, the center of the bootstrap sampling distribution will be near $\\hat{\\theta}$ because the bootstrap population consists of the observed data. \n",
    "So, rather than compute the chance of a value at least as large as the observed statistic, we find the chance of a value at least as small as 0.\n",
    "\n",
    "**The hypothesized value of 0 is far from the sampling distribution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dfadab-5bd4-41e8-a96c-8a891ffb8833",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([elem for elem in bs_theta2 if elem < 0.0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf85a4-2abc-4b0e-b69c-6c667512f464",
   "metadata": {},
   "source": [
    "None of the 1000 simulated regression coefficients are as small as the hypothesized coefficient. Statistical logic leads us to **reject the null hypothesis that the true association of humidity and air quality is 0.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f195c0-21dd-4e04-a8b2-343691f4e6c8",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## The Snowy Plover\n",
    "\n",
    "This example borrows some wording from Spring 2020's Data 100, Lecture 22."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ac023-de6d-4ad8-8b0a-dec29f0caf00",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The Data \n",
    "The [Snowy Plover](https://www.audubon.org/field-guide/bird/snowy-plover) is a tiny bird that lives on the coast in parts of California and elsewhere. It is so small that it is vulnerable to many predators and to people and dogs that don't look where they are stepping when they go to the beach. It is considered endangered in many parts of the US.\n",
    "\n",
    "The data are about the eggs and newly-hatched chicks of the Snowy Plover. Here's a [parent bird and some eggs](http://cescos.fau.edu/jay/eps/articles/snowyplover.html).\n",
    "\n",
    "![plover and eggs](images/plover_eggs.jpg)\n",
    "\n",
    "The data were collected at the Point Reyes National Seashore by a former [student at Berkeley](https://openlibrary.org/books/OL2038693M/BLSS_the_Berkeley_interactive_statistical_system). The goal was to see how the size of an egg could be used to predict the weight of the resulting chick. The bigger the newly-hatched chick, the more likely it is to survive.\n",
    "\n",
    "![plover and chick](images/plover_chick.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9c954e-1c2a-4484-bf35-60091200812b",
   "metadata": {},
   "source": [
    "Each row of the data frame below corresponds to one Snowy Plover egg and the resulting chick. Note how tiny the bird is:\n",
    "\n",
    "* Egg Length and Egg Breadth (widest diameter) are measured in millimeters\n",
    "* Egg Weight and Bird Weight are measured in grams; for comparison, a standard paper clip weighs about one gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd944f7f-86ed-41a9-8ac6-65a90e0534fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "eggs = pd.read_csv('data/snowy_plover.csv.gz')\n",
    "eggs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f341a-365a-4a33-8dce-91103cce8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "eggs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74117d04-d63b-4461-bc96-614805e8379f",
   "metadata": {},
   "source": [
    "For a particular egg, $x$ is the vector of length, breadth, and weight. The proposed model is\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{Newborn weight}} = \\hat{\\theta}_0 + \\hat{\\theta}_1 \\text{egg\\_weight} + \\hat{\\theta}_2 \\text{egg\\_length} + \\hat{\\theta}_2 \\text{egg\\_breadth}\n",
    "$$\n",
    "\n",
    "Let's fit this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b67df-6693-443b-92db-2787dd87c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = eggs[\"bird_weight\"]\n",
    "X = eggs[[\"egg_weight\", \"egg_length\", \"egg_breadth\"]]\n",
    "    \n",
    "model = lm.LinearRegression(fit_intercept=True).fit(X, y)\n",
    "\n",
    "display(pd.DataFrame(\n",
    "    [model.intercept_] + list(model.coef_),\n",
    "    columns=['theta_hat'],\n",
    "    index=['intercept', 'egg_weight', 'egg_length', 'egg_breadth']))\n",
    "\n",
    "all_features_rmse = np.mean((y - model.predict(X)) ** 2)\n",
    "\n",
    "print(\"RMSE\", all_features_rmse)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088fd519-afd6-4abe-a567-63137f28b984",
   "metadata": {},
   "source": [
    "Let's try bootstrapping the sample to obtain a 95% confidence intervals for all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f04a1-2de7-43f3-8f14-667ed4add13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a list of the coefficients of the fitted model\n",
    "def all_thetas(sample):\n",
    "    model = lm.LinearRegression().fit(\n",
    "        sample[[\"egg_weight\", \"egg_length\", \"egg_breadth\"]],\n",
    "        sample[\"bird_weight\"])\n",
    "    return [model.intercept_] + model.coef_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb03046",
   "metadata": {},
   "source": [
    "We can re-use our bootstrapping function from before to get synthetic estimates of all of the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3cbc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_thetas = pd.DataFrame(\n",
    "    bootstrap(eggs, all_thetas, 1000), \n",
    "    columns=['intercept', 'egg_weight', 'egg_length', 'egg_breadth'])\n",
    "\n",
    "bs_thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa6658f",
   "metadata": {},
   "source": [
    "Computing the confidence intervals using the 1,000 synthetic estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78119799",
   "metadata": {},
   "outputs": [],
   "source": [
    "cis = (bs_thetas\n",
    "       .apply(bootstrap_ci).T\n",
    "       .rename(columns={0: 'lower', 1: 'upper'}))\n",
    "cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_coeffs(bs_thetas, rows, cols):\n",
    "    cis = (bs_thetas\n",
    "       .apply(bootstrap_ci).T\n",
    "       .rename(columns={0: 'lower', 1: 'upper'}))\n",
    "    display(cis)\n",
    "    from plotly.subplots import make_subplots\n",
    "    fig = make_subplots(rows=rows, cols=cols, subplot_titles=cis.index)\n",
    "    for i, coeff_name in enumerate(cis.index):\n",
    "        c = (i % cols) + 1\n",
    "        r = (i // cols) + 1\n",
    "        fig.add_histogram(x=bs_thetas[coeff_name], name=coeff_name, \n",
    "                        row=r, col=c, histnorm='probability')\n",
    "        fig.add_vline(x=0, row=r, col=c)\n",
    "        fig.add_vline(x=cis.loc[coeff_name, 'lower'], line=ci_line_style, \n",
    "                      row=r, col=c)\n",
    "        fig.add_vline(x=cis.loc[coeff_name, 'upper'], line=ci_line_style, \n",
    "                      row=r, col=c)\n",
    "    return fig\n",
    "\n",
    "visualize_coeffs(bs_thetas, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593bbfa2",
   "metadata": {},
   "source": [
    "Because all the confidence intervals contain 0, we cannot reject the null hypothesis that the true coefficient on each term is 0.\n",
    "\n",
    "Does this mean that all the parameters are statistically indistinguishable from 0?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a7309-4e8f-4c1b-83aa-77919ec9738a",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Inspecting the Relationship between Features\n",
    "\n",
    "To see what's going on, we'll make a scatter plot matrix for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56515ba-11b8-496d-a630-f9740cf98e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_matrix(eggs, width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69b4b4-896b-4702-abbf-ce50bac91ad9",
   "metadata": {},
   "source": [
    "This shows that `bird_weight`\n",
    "is highly correlated with all the other\n",
    "variables (the bottom row), which means fitting a linear model is a good idea.\n",
    "\n",
    "But, we also see that `egg_weight` is highly correlated with all the variables\n",
    "(the top row).\n",
    "We saw in lecture that this could result in high variance coefficients and harm inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9358610-de9c-4775-b596-fa7a5543d282",
   "metadata": {},
   "source": [
    "Here are the numeric correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f401aba9-5745-4a5d-89bc-6eac07980eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(eggs.corr().round(2), text_auto=True, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c7353f-0055-46c1-8710-305e957c4007",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Changing Our Modeling Features\n",
    "\n",
    "Based on the correlations above, `egg_weight` looks like the strongest predictor of newborn chick weight.\n",
    "\n",
    "An SLR model with just `egg_weight` performs almost as well as the model that uses all three variables, and the confidence interval for $\\hat{\\theta}_1$ no longer \n",
    "contains zero.\n",
    "\n",
    "- Note that the model with additional variables has a slightly lower RMSE! In sample RMSE will never go up when you add features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c8d7d-9718-48c9-80a6-cf9a1eb86714",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = eggs[\"bird_weight\"]\n",
    "X = eggs[[\"egg_weight\"]]\n",
    "    \n",
    "model = lm.LinearRegression(fit_intercept=True).fit(X, y)\n",
    "\n",
    "display(pd.DataFrame([model.intercept_] + list(model.coef_),\n",
    "             columns=['theta_hat'],\n",
    "             index=['intercept', 'egg_weight']))\n",
    "print(\"All Features RMSE: \", all_features_rmse)\n",
    "print(\"Simpler model RMSE: \", np.mean((y - model.predict(X)) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e383f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of the intercept and slope of the SLR model using just egg_weight\n",
    "def egg_weight_coeff(sample):\n",
    "    model = lm.LinearRegression().fit(\n",
    "        sample[[\"egg_weight\"]],\n",
    "        sample[\"bird_weight\"])\n",
    "    return [model.intercept_] + model.coef_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0cb9e3",
   "metadata": {},
   "source": [
    "We can re-use our `bootstrap` function from earlier to generate synthetic estimates of the coefficient on `egg_weight`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d89806",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_thetas_egg_weight = pd.DataFrame(\n",
    "    bootstrap(eggs, egg_weight_coeff, 1000), \n",
    "    columns=['intercept', 'egg_weight'])\n",
    "bs_thetas_egg_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_coeffs(bs_thetas_egg_weight, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d131bdb7-36dd-45af-a66c-d1e097e65b04",
   "metadata": {},
   "source": [
    "Notice how much tighter the confidence interval is for `egg_weight` above, relative to the regression where we included all three collinear terms!\n",
    "\n",
    "As this example shows, checking for collinearity is important for inference (and less so for prediction).\n",
    "\n",
    "When we fit a model on highly correlated variables, confidence intervals can have high variance, preventing us from making meaningful statistical conclusions about the relationships between features and outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
