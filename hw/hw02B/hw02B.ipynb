{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw02B.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro-hw2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# üïµÔ∏è Homework 2B: Food Safety (Continued)\n",
    "\n",
    "## Due Date: Thursday, February 13, 11:59 PM\n",
    "You must submit this assignment to Gradescope by the on-time deadline, Thursday, February 13, 11:59 PM. Please read the syllabus for the Slip Day policy. No late submissions beyond what is outlined in the Slip Day policy will be accepted. **We strongly encourage you to plan to submit your work to Gradescope several hours before the stated deadline.** This way, you will have ample time to reach out to staff for support if you encounter difficulties with submission. While course staff is happy to help guide you with submitting your assignment ahead of the deadline, we will not respond to last-minute requests for assistance (TAs need to sleep, after all!).\n",
    "\n",
    "Please read the instructions carefully when you are submitting your work to Gradescope.\n",
    "\n",
    "## üë• Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the homework, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *list collaborators here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìù This Assignment\n",
    "\n",
    "In this homework, we will continue our exploration of restaurant food safety scores for restaurants in San Francisco. The main goal for this assignment is to focus more on the analysis of the dataset, building on the data cleaning we have done earlier in HW 2A. \n",
    "\n",
    "\n",
    "After this homework, you should be comfortable with:\n",
    "* Reading `pandas` documentation and using `pandas` methods,\n",
    "* Working with data at different levels of granularity,\n",
    "* Using `groupby` with different aggregation functions, and\n",
    "* Chaining different `pandas` functions and methods to find answers to exploratory questions.\n",
    "\n",
    "\n",
    "## Score Breakdown \n",
    "Question | Manual | Points\n",
    "--- | --- | ---\n",
    "1a | no | 2\n",
    "1b | no | 3\n",
    "1c | no | 3\n",
    "2a | no | 2\n",
    "2b | no | 3\n",
    "2c | no | 3\n",
    "3a | yes | 4\n",
    "3b | yes | 4\n",
    "4a | no | 2\n",
    "4b | no | 2\n",
    "4c | no | 3\n",
    "4d | no | 3\n",
    "4e | yes | 1\n",
    "Total | 3 | 35\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèéÔ∏è Before You Start\n",
    "\n",
    "For each question in the assignment, please write down your answer in the answer cell(s) right below the question. \n",
    "\n",
    "We understand that it is helpful to have extra cells breaking down the process towards reaching your final answer. If you happen to create new cells below your answer to run code, **NEVER** add cells between a question cell and the answer cell below it. It will cause errors when we run the autograder, and it will sometimes cause a failure to generate the PDF file.\n",
    "\n",
    "**Important note: The local autograder tests will not be comprehensive. You can pass the automated tests in your notebook but still fail tests in the autograder.** Please be sure to check your results carefully.\n",
    "\n",
    "Finally, unless we state otherwise, **do not use for loops or list comprehensions**. The majority of this assignment can be done using built-in commands in `pandas` and `NumPy`. Our autograder isn't smart enough to check, but you're depriving yourself of key learning objectives if you write loops / comprehensions, and you also won't be ready for the midterm.\n",
    "\n",
    "### üêõ Debugging Guide\n",
    "If you run into any technical issues, we highly recommend checking out the [Data 100 Debugging Guide](https://ds100.org/debugging-guide/). In this guide, you can find general questions about Jupyter notebooks / Datahub, Gradescope, and common `pandas` errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HW 2A, we took you through the entire process of reading data from a file to perform some exploration of the data. Here, we again load the dataset that we will be using in HW 2B along with some of the columns we had added in HW 2A. For any additional context regarding the dataset, we encourage you to revisit HW 2A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus = pd.read_csv('data/bus.csv', encoding='ISO-8859-1').rename(columns={\"business id column\": \"bid\"})\n",
    "bus['postal5'] = bus['postal_code'].str[:5]\n",
    "ins = pd.read_csv('data/ins.csv')\n",
    "ins['timestamp'] = pd.to_datetime(ins['date'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "ins['bid'] = ins['iid'].str.split(\"_\", expand=True)[0].astype(int) \n",
    "\n",
    "# This code is essential for the autograder to function properly. Do not edit.\n",
    "ins_test = ins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "business-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "# üîé Question 1: Inspecting the Inspections\n",
    "\n",
    "## üöÄ Question 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-174ed23c543ad9da",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let's start by looking again at the first 5 rows of `ins` to see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f0fbe724a2783e33",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ins.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To better understand how the scores have been allocated, let's examine how the maximum score varies for each type of inspection. \n",
    "\n",
    "Create a `DataFrame` object `ins_score_by_type`, indexed by all the inspection types (e.g., New Construction, Routine - Unscheduled, etc.), with a single column named `max_score` containing the highest score received. Additionally, order `ins_score_by_type` by `max_score` in descending order. \n",
    "\n",
    "**Hint:** You may find the `rename` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html)) to be useful! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ins_score_by_type = ...\n",
    "ins_score_by_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Question 1b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Given the variability of `ins['score']` observed in 1a, let's examine the inspection scores `ins['score']` further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ins['score'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "There are a large number of inspections with a score of -1. These are probably missing values. Let's see what types of inspections have scores and which do not (score of -1). \n",
    "\n",
    "- First, define a new column `Missing Score` in `ins` where each row maps to the string `\"Yes\"` if the `score` for that business is -1 and `\"No\"` otherwise. \n",
    "\n",
    "- Then, use `groupby` to find the number of inspections for every combination of `type` and `Missing Score`. Store these values in a new column `Count`. \n",
    "\n",
    "- Finally, sort `ins_missing_score_group` by descending `Count`s. \n",
    "The result should be a `DataFrame` that looks like the one shown below.\n",
    "\n",
    "**Hint**: You may find the `map` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.map.html)) useful for defining `Missing Score`! \n",
    "\n",
    "<table border=\"1\" class=\"dataframe\" >  <thead>    \n",
    "    <tr style=\"text-align: right;\">      <th></th>      <th></th>      <th>Count</th>   </tr>\n",
    "    <tr style=\"text-align: right;\">      <th>type</th>      <th>Missing Score</th>      <th></th>   </tr>    <tr align=\"right\"> <tbody>    \n",
    "    <tr  align=\"right\">      <th>Routine - Unscheduled</th>      <th>No</th>      <td>14031</td>         </tr>    \n",
    "    <tr  align=\"right\">      <th>...</th>      <td>...</td>      <td>...</td>        </tr>    \n",
    "    <tr  align=\"right\">      <th>...</th>      <td>...</td>      <td>...</td>       </tr>    </tbody> </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ins['Missing Score'] = ...\n",
    "ins_missing_score_group = ...\n",
    "\n",
    "\n",
    "ins_missing_score_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Question 1c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `groupby` to perform the analysis above gave us a `DataFrame` that wasn't the most readable at first glance. There are better ways to represent the information above that take advantage of the fact that we are looking at combinations of two variables. It's time to pivot (pun intended)!\n",
    "\n",
    "Create a `DataFrame` that looks like the one below, and assign it to the variable `ins_missing_score_pivot`. \n",
    "\n",
    "You'll want to use the `pivot_table` method of the `DataFrame` class, which you can read about in the `pivot_table` [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot_table.html). \n",
    "\n",
    "- Once you create `ins_missing_score_pivot`, add another column titled `Percentage Missing`, which contains the proportion of missing scores within each `type`. \n",
    "\n",
    "- Then, sort `ins_missing_score_pivot` in ascending order of `Percentage Missing`. Reassign the sorted `DataFrame` back to `ins_missing_score_pivot`.\n",
    "\n",
    "**Hint:** Consider what happens if no values correspond to a particular combination of `Missing Score` and `type`. Looking at the documentation for `pivot_table`, is there any function argument that allows you to specify what value to fill in?\n",
    "\n",
    "If you've done everything right, you should observe that inspection scores appear only to be assigned to `Routine - Unscheduled` inspections and that `ins_missing_score_pivot` looks like the `DataFrame` below:\n",
    "\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\" >  <thead>    \n",
    "    <tr style=\"text-align: right;\">      <th>Missing Score</th>      <th>No</th>      <th>Yes</th>      <th>Percentage Missing</th>    </tr>    <tr style=\"text-align: right;\">      <th>type</th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    \n",
    "    <tr  align=\"right\">      <th>Routine - Unscheduled</th>      <td>14031</td>      <td>46</td>      <td>0.003268</td>    </tr>    \n",
    "    <tr  align=\"right\">      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    \n",
    "    </tbody></table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ins_missing_score_pivot = ...\n",
    "\n",
    "...\n",
    "\n",
    "ins_missing_score_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that inspection scores appear only to be assigned to `Routine - Unscheduled` inspections. Also, it is reasonable for inspection types such as `New Ownership` and `Complaint` to have no associated inspection scores, but you might be curious why there are no inspection scores for the `Reinspection/Followup` inspection type. Later in the HW, we will examine these `Reinspection/Followup` inspections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "business-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Question 2: Joining Data Across Tables\n",
    "\n",
    "In this question, we will start to connect data across multiple tables. We will be using the `pd.merge` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "--- \n",
    "\n",
    "## üöÄ Question 2a\n",
    "\n",
    "Let's figure out which restaurants had the lowest scores. Before we proceed, filter out missing scores from `ins` so that negative scores don't influence our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = ins[ins[\"score\"] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We'll start by creating a new `DataFrame` called `ins_named`. `ins_named` should be exactly the same as `ins`, except that it should have the name and address of every business, as determined by the `bus` `DataFrame`. \n",
    "\n",
    "**Hint**: Use the `DataFrame` method `merge` to join the `ins` `DataFrame` with the appropriate portion of the `bus` `DataFrame`. See the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html) for guidance on how to use `merge` function to combine two `DataFrame` objects. The first few rows of `ins_named` `DataFrame` are shown below:\n",
    "\n",
    "<img src=\"pics/2a.png\" width=\"1080\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "ins_named.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "--- \n",
    "\n",
    "## üöÄ Question 2b\n",
    "\n",
    "Look at the 20 businesses in `ins_named` with the lowest scores. Order `ins_named` by each business's minimum score in ascending order. Use the business names in ascending order to break ties. The resulting `DataFrame` should look like the table below.\n",
    "\n",
    "This one is pretty challenging! Don't forget to rename the `score` column. \n",
    "\n",
    "**Hint**: The `agg` function can accept a dictionary as an input. See the `agg` [documentation](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.core.groupby.DataFrameGroupBy.agg.html). Additionally, when thinking about what aggregation functions to use, ask yourself what value would be in the `name` column for each entry across the group? Can we select just one of these values to represent the whole group?\n",
    "\n",
    "As usual, **YOU SHOULD NOT USE LOOPS OR LIST COMPREHENSIONS**. Try to break down the problem piece by piece instead, gradually chaining together different `pandas` functions. Feel free to use more than one line!\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    \n",
    "    <tr style=\"text-align: right;\">      <th></th>      <th>name</th>      <th>min score</th>    </tr> \n",
    "    <tr  align=\"right\">  <th align=\"right\">bid</th>      <th></th>      <th></th>    </tr> </thead>  <tbody>    \n",
    "    <tr  align=\"right\">      <th>86718</th>      <td>Lollipot</td>      <td>45</td>    </tr>  \n",
    "    <tr  align=\"right\">      <th>...</th>      <td>...</td>      <td>...</td>    </tr> \n",
    "  </tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "twenty_lowest_scoring = ... \n",
    "\n",
    "# DO NOT USE LIST COMPREHENSIONS OR LOOPS OF ANY KIND!!!\n",
    "\n",
    "...\n",
    "\n",
    "twenty_lowest_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "--- \n",
    "## üöÄ Question 2c\n",
    "Let's do some more interesting analysis with our lowest score calculations. In the cell below, assign `worst_3_inspection_restaurants` to a two-column `DataFrame` with 15 rows.\n",
    "\n",
    "- One column is the `name` of each business.\n",
    "\n",
    "- The other column is a modified average inspection score of each business called `lowest 3 average`. \n",
    "\n",
    "    - To calculate `lowest 3 average`, find the average of each business's **three lowest inspection scores**. \n",
    "\n",
    "    - If a business has less than three inspection scores, take the average of all of its inspection scores (i.e., either one or two scores). \n",
    "\n",
    "- Finally, assign `worst_3_inspection_restaurants` to a `DataFrame` of the 15 rows with the lowest `lowest 3 average`, sorted by `lowest 3 average` ascending.  \n",
    "\n",
    "`worst_3_inspection_restaurants` should look like the one below.\n",
    "\n",
    "**Hint**: 2b‚Äôs advice also applies here! Furthermore, your answer to 2b may be helpful as a starting point. This question is intentionally left open-ended, so feel free to use any combination of `pandas` functions found online. Similarly to 2b, do not use loops or list comprehensions. Use as many lines as you see fit, so long as your final answer is saved to `worst_3_inspection_resturants`. For isolating each business's lowest three inspection scores, it may be useful to know that when `groupby` applies aggregating functions, it preserves the sort order of the inputted `DataFrame`. \n",
    "\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    \n",
    "    <tr style=\"text-align: right;\">      <th></th>      <th>name</th>      <th>lowest 3 average</th>    </tr> \n",
    "    <tr  align=\"right\">  <th align=\"right\">bid</th>      <th></th>      <th></th>    </tr> </thead>  <tbody>    \n",
    "    <tr  align=\"right\">      <th>84590</th>      <td>Chaat Corner</td>      <td>54.0</td>    </tr>  \n",
    "    <tr  align=\"right\">      <th>...</th>      <td>...</td>      <td>...</td>    </tr> \n",
    "  </tbody></table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "worst_3_inspection_restaurants = ...\n",
    "\n",
    "...\n",
    "worst_3_inspection_restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "# üåÆ Question 3: `pandas` Potpourri\n",
    "\n",
    "In this question, we ask you to describe `pandas` operations and explain specific concepts using `ins_named`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## üåÆ Question 3a\n",
    "\n",
    "Consider the chained `pandas` statement below:\n",
    "\n",
    "`q3a_df = ins_named[ins_named[\"name\"].str.lower().str.contains(\"taco\")].groupby(\"bid\").filter(lambda sf: sf[\"score\"].max() > 95).agg(\"count\")`\n",
    "\n",
    "We can decompose this statement into three parts:\n",
    "\n",
    "```\n",
    "temp1 = ins_named[ins_named[\"name\"].str.lower().str.contains(\"taco\")]\n",
    " \n",
    "temp2 = temp1.groupby(\"bid\").filter(lambda sf: sf[\"score\"].max() > 95)\n",
    " \n",
    "q3a_df = temp2.agg(\"count\")\n",
    "```\n",
    "\n",
    "For each line of code above, write one sentence describing what the line of code accomplishes. Feel free to create a cell to see what each line does. In total, you'll write three sentences.\n",
    "\n",
    "Finally, write an example homework question whose answer is `q3a_df`. \n",
    "\n",
    "- This example homework question should only be one sentence.\n",
    "\n",
    "**Note: While the first part of this question will be graded for correctness, the second part is a bit more open-ended. Answers that demonstrate correct understanding will receive full credit.** \n",
    "\n",
    "An example answer will look like the following: \"`temp1` creates a ... `temp2` transforms `temp1` by ... Finally, `q3a_df` results in a `DataFrame` that ... A question that is answered by this chain of operations is ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## üåÆ Question 3b\n",
    "\n",
    "Consider `ins_named`, `temp1`, `temp2`, and `q3a_df` from the previous problem. What is the granularity of each `DataFrame`? Explain your answer in no more than four sentences.\n",
    "\n",
    "**Note**: For more details on what the granularity of a `DataFrame` means, feel free to check the [course notes](https://ds100.org/course-notes/eda/eda.html#granularity)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Question 4: Missing Inspections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our inspection data, we are given the `type` of each inspection. These categories were lightly investigated in Question 1, centered on the number of missing scores within each `type`. Since the `timestamp` and `score` for each inspection are also provided, we can do a more interesting analysis relating the `score` and `timestamp` of specific types of inspections. \n",
    "\n",
    "Specifically, in Question 4, we are interested in the possible relationship between inspections of the `type` \"Routine - Unscheduled\" and \"Reinspection/Followup\" (the two most frequent inspection types in our dataset). We might guess that a follow-up (\"Reinspection/Followup\") inspection occurs more frequently when an initial (\"Routine - Unscheduled\") inspection receives a low score. To confirm this hunch, let‚Äôs investigate the rate of follow-up inspections for different initial scores. To simplify your analysis, we have provided a new `DataFrame` (`reinspections`). \n",
    "\n",
    "- `reinspections` contains every \"Routine - Unscheduled\" inspection, along with the relevant `bid` and `name` associated with the initial inspection. \n",
    "- `routine timestamp` indicates when the initial inspection occurred. \n",
    "- `routine score` is the score that the initial inspection received. \n",
    "- `day difference` is the number of days between the initial inspection and a follow-up inspection if done within one year. \n",
    "    \n",
    "Some initial inspections did not have any follow-up inspections within one year. In these cases, `day difference` is assigned a filler value of -1.\n",
    "\n",
    "Run the cell below to load in `reinspections`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinspections = pd.read_csv('data/reinspections.csv')\n",
    "reinspections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "--- \n",
    "## üöÄ Question 4a\n",
    "First, create a new `Boolean` column `recent reinspection?` that indicates whether a follow-up inspection occurred within 62 days inclusive (~2 months) of an initial inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reinspections['recent reinspection?'] = ...\n",
    "\n",
    "reinspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "--- \n",
    "## üöÄ Question 4b\n",
    "To simplify our analysis, let‚Äôs assign `routine score`s to buckets. Buckets are similar to the bins of a histogram. Each bucket contains all scores that fall in a particular range.\n",
    "\n",
    "Below we have defined the function `bucketify`. Use `bucketify` to create a new column in `reinspections` called `score buckets` that **maps** the score of an initial inspection to one of these predefined buckets.\n",
    "\n",
    "**Hint:** You may find the `map` [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.map.html) useful. Alternatively, see [this demonstration](https://ds100.org/course-notes/pandas_3/pandas_3.html#approach-3-sorting-using-the-map-function) in the course notes for an example use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bucketify(score):\n",
    "    if score < 65: \n",
    "        return '0 - 65'\n",
    "    elif score < 70:\n",
    "        return '65 - 69'\n",
    "    elif score < 75:\n",
    "        return '70 - 74'\n",
    "    elif score < 80:\n",
    "        return '75 - 79'\n",
    "    elif score < 85:\n",
    "        return '80 - 74'\n",
    "    elif score < 90:\n",
    "        return '85 - 89'\n",
    "    elif score < 95:\n",
    "        return '90 - 94'\n",
    "    else:\n",
    "        return '95 - 100'\n",
    "        \n",
    "reinspections['score buckets'] = ...\n",
    "\n",
    "reinspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "--- \n",
    "## üöÄ Question 4c\n",
    "Before we complete our analysis, remove all rows whose `score buckets` contain less than 125 rows. Assign `reinspection_filtered` to this new `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reinspections_filtered = ...\n",
    "\n",
    "reinspections_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "--- \n",
    "## üöÄ Question 4d\n",
    "\n",
    "To conclude our analysis, use `resinpsections_filtered` to generate a `DataFrame` with the **proportion** of initial inspections within each bucket that were reinspected within 62 days, along with the total **count** of initial inspections included in each bucket. Sort this `DataFrame` by ascending counts. Assign this new `DataFrame` to `reinspection_proportions`.\n",
    "\n",
    "`reinspection_proportions` should look like the `DataFrame` below.\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\" >  <thead>    \n",
    "    <tr style=\"text-align: right;\">      <th></th>      <th>recent reinspection?</th>   <th></th> </tr>    \n",
    "    <tr style=\"text-align: right;\">      <th></th>      <th>proportion</th>      <th>count</th>    </tr>    \n",
    "    <tr style=\"text-align: right;\">      <th>score buckets</th>      <th></th>      <th></th>     </tr>  </thead>  <tbody>    \n",
    "    <tr  align=\"right\">      <th>70 - 74</th>      <td>0.407821</td>      <td>358</td>    </tr>    \n",
    "    <tr  align=\"right\">      <th>...</th>      <td>...</td>      <td>...</td>    </tr>    \n",
    "    </tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reinspection_proportions = ...\n",
    "\n",
    "reinspection_proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "--- \n",
    "## üöÄ Question 4e\n",
    "\n",
    "Do you notice any trends? Are your results consistent with your prior knowledge about restaurants that receive high or low health inspection scores? Answer in the cell below.\n",
    "\n",
    "**This question is graded on effort, there is no one \"correct\" answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Summary of Inspections Data\n",
    "\n",
    "We have done a lot in this homework! \n",
    " \n",
    "- Broke down the inspection scores in detail using `groupby` and `pivot_table`.\n",
    "- Joined the business and inspection data and identified restaurants with the worst ratings.\n",
    "- Took a deep dive into understanding any trends between an inspection score and reinspection frequency.\n",
    "\n",
    "Over the course of this 2-part homework, we hope you have become more familiar with `pandas` - in terms of identifying when to use particular functions, how they work, when they can support EDA - as well as with EDA and Data Cleaning, as part of the broader Data Science Lifecycle. These tools will serve you well as a data scientist!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have finished Homework 2B! ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coco and Oreo say hi :)\n",
    "\n",
    "<img src = \"pics/IMG_2887.jpg\" width = \"400\" class=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Content Feedback\n",
    "\n",
    "If you have any feedback about this assignment or about any of our other weekly, weekly assignments, lectures, or discussions, please fill out the [Course Content Feedback Form](https://forms.gle/Yc3kdzNLPsVKNz2g6). Your input is valuable in helping us improve the quality and relevance of our content to better meet your needs and expectations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Once you submit this file to the HW 2B Coding assignment on Gradescope, Gradescope will automatically submit a PDF file with your written answers to the HW 2B Written assignment. If you run into any issues when running this cell, feel free to check this [section](https://ds100.org/debugging-guide/autograder_gradescope/autograder_gradescope.html#why-does-grader.exportrun_teststrue-fail-if-all-previous-tests-passed) in the Data 100 Debugging Guide.\n",
    "\n",
    "**Important**: Please check that your written responses were generated and submitted correctly to the HW 2B Written Assignment.\n",
    "\n",
    "**You are responsible for ensuring your submission follows our requirements and that the PDF for HW 2B written answers was generated/submitted correctly. We will not be granting regrade requests nor extensions to submissions that don't follow instructions.** If you encounter any difficulties with submission, please don't hesitate to reach out to staff prior to the deadline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "otter": {
   "OK_FORMAT": true,
   "require_no_pdf_confirmation": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(len(ins_score_by_type.columns) == 1)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(ins_score_by_type.columns[0] == 'max_score')\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(set(ins_score_by_type.index) == set(ins['type'].unique()))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(('New Ownership - Followup', 'Yes') in ins_missing_score_group.index)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(ins_missing_score_group.columns[0] == 'Count')\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(len(ins_missing_score_group) == 16)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(type(ins_missing_score_pivot) == pd.DataFrame)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 'name' in ins_named and 'address' in ins_named\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> ins_named[ins_named['Missing Score'] == True].shape[0] == 0\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> ins_named.shape == (14031, 9)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> ins_named.reset_index()['date'].equals(ins[ins['score'] > 0].reset_index()['date'])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert twenty_lowest_scoring.shape == (20, 2)\n>>> assert len(twenty_lowest_scoring.index.names) == 1\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> twenty_lowest_scoring.iloc[0][0] == 'Lollipot'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert worst_3_inspection_restaurants.shape == (15, 2)\n>>> assert len(worst_3_inspection_restaurants.index.names) == 1\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(worst_3_inspection_restaurants.iloc[0][0] == 'Chaat Corner')\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert reinspections.shape == (14077, 6)\n>>> assert (reinspections.columns == ['bid', 'routine timestamp', 'routine score', 'name', 'day difference', 'recent reinspection?']).all()\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert reinspections.shape == (14077, 7)\n>>> assert (reinspections.columns == ['bid', 'routine timestamp', 'routine score', 'name', 'day difference', 'recent reinspection?', 'score buckets']).all()\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4c": {
     "name": "q4c",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert (reinspections_filtered.columns == ['bid', 'routine timestamp', 'routine score', 'name', 'day difference', 'recent reinspection?', 'score buckets']).all()\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4d": {
     "name": "q4d",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert reinspection_proportions.shape == (6, 2)\n>>> assert reinspection_proportions.columns[0] == ('recent reinspection?', 'proportion')\n>>> assert reinspection_proportions.columns[1] == ('recent reinspection?', 'count')\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
